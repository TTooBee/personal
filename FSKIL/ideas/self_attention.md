---
id: self_attention
aliases: []
tags: []
---

# self_attention

## 2025-03-10

CKA loss cannot consider the position of the vectors, so maybe it'll be good to make the vectors contain the information about the context, and this will be done by self_attention mechanism similar to the one in the Transformer.
The number of parameters of the Transformer encoder is greater than I thought. Could be larger than BC-ResNet itself..?
One more thing.
